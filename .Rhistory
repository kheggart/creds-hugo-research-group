profile=character(),
twitter=character(),
stringsAsFactors=FALSE)
positions <- c(grep("(<h4>)\\w+",unlist(list.members[1])),length(unlist(list.members[1]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
#This function is hideous but it works
extract <- function(x,y) {
#x <- positions[8] #for testing
#y <- positions[9] #for testing
#counter <- x-5
#add membership group
group <- "blank"
#Add their name
name <- str_replace_all(members[x],c("<h4>" = "", "</h4>" = ""))
#Then set the position to one back from the heading, and to 2 back from the NEXT heading
x <- x-1
y <- y-2
#And extract just the relevant chunk for this member
raw <- members[x:y]
#Grab the picture if there is one
pic <- ifelse(grepl("img",members[x]),members[x],paste0("no pic"))
#Find the p bio, and grab that
index <- grep("<p>",raw)
bio <- c(raw[c(index)])
bio <- paste(bio, collapse="\n")
bio <- html_text(read_html(bio))
short_bio <- str_extract(bio, '.*?[a-z0-9][.?!](?= )')
#Find the bit with the links in it
index <- head(grep("<ul>",raw),1) #grab the first place it occurs
index_end <- ifelse(length(grep("</ul>",raw))<index,length(raw),tail(grep("</ul>",raw))) #grab the last place it occurs
links <- ifelse(length(index)>0,c(raw[c(index:index_end)]),"")
index <- ifelse(length(links)>0,grep("<a href", links), "")
links <- ifelse(length(index)<1, "",
read_html(unlist(links[[index]])) %>%
html_nodes("a") %>%
html_attr("href")
)
#Find the UTS profile, href for UTS
index <- grep("uts.edu.au",links)
profile <- ifelse(length(index)<1,paste0(""),links[c(index)])
#Find the twitter handle, href for twitter
index <- grep("twitter",links)
twitter <- ifelse(length(index)<1,paste0(""),links[c(index)])
#and then return all of that...this is VERY inelegant but it should work
person <- as.data.frame(cbind(group,name,pic,bio,short_bio,profile,twitter))
person %>%
mutate(across(everything(), as.character))
overall <<- rbind(overall,person)
}
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
positions <- c(grep("(<h4>)\\w+",unlist(list.members[1])),length(unlist(list.members[1]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
#This function is hideous but it works
extract <- function(x,y) {
#x <- positions[8] #for testing
#y <- positions[9] #for testing
#counter <- x-5
#add membership group
group <- "blank"
#Add their name
name <- str_replace_all(members[x],c("<h4>" = "", "</h4>" = ""))
#Then set the position to one back from the heading, and to 2 back from the NEXT heading
x <- x-1
y <- y-2
#And extract just the relevant chunk for this member
raw <- members[x:y]
#Grab the picture if there is one
pic <- ifelse(grepl("img",members[x]),members[x],paste0("no pic"))
#Find the p bio, and grab that
index <- grep("<p>",raw)
bio <- c(raw[c(index)])
bio <- paste(bio, collapse="\n")
bio <- html_text(read_html(bio))
short_bio <- str_extract(bio, '.*?[a-z0-9][.?!](?= )')
#Find the bit with the links in it
index <- head(grep("<ul>",raw),1) #grab the first place it occurs
index_end <- ifelse(length(grep("</ul>",raw))<index,length(raw),tail(grep("</ul>",raw))) #grab the last place it occurs
links <- ifelse(length(index)>0,c(raw[c(index:index_end)]),"")
index <- grep("<a href", links)  #ifelse(length(links)>0,grep("<a href", links), "")
links <- ifelse(length(index)<1, "",
read_html(unlist(links[[index]])) %>%
html_nodes("a") %>%
html_attr("href")
)
#Find the UTS profile, href for UTS
index <- grep("uts.edu.au",links)
profile <- ifelse(length(index)<1,paste0(""),links[c(index)])
#Find the twitter handle, href for twitter
index <- grep("twitter",links)
twitter <- ifelse(length(index)<1,paste0(""),links[c(index)])
#and then return all of that...this is VERY inelegant but it should work
person <- as.data.frame(cbind(group,name,pic,bio,short_bio,profile,twitter))
person %>%
mutate(across(everything(), as.character))
overall <<- rbind(overall,person)
}
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
overall$group <- c(rep("member",length(grep("(<h4>)\\w+",unlist(list.members[1]))))) #add member group for n of people in it
positions <- c(grep("(<h4>)\\w+",unlist(list.members[2])),length(unlist(list.members[2]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
#This function is hideous but it works
extract <- function(x,y) {
#x <- positions[8] #for testing
#y <- positions[9] #for testing
#counter <- x-5
#add membership group
group <- "blank"
#Add their name
name <- str_replace_all(members[x],c("<h4>" = "", "</h4>" = ""))
#Then set the position to one back from the heading, and to 2 back from the NEXT heading
x <- x-1
y <- y-2
#And extract just the relevant chunk for this member
raw <- members[x:y]
#Grab the picture if there is one
pic <- ifelse(grepl("img",members[x]),members[x],paste0("no pic"))
#Find the p bio, and grab that
index <- grep("<p>",raw)
bio <- c(raw[c(index)])
bio <- paste(bio, collapse="\n")
bio <- html_text(read_html(bio))
short_bio <- str_extract(bio, '.*?[a-z0-9][.?!](?= )')
#Find the bit with the links in it
ifelse(grepl("<ul>",raw), {
index <- head(grep("<ul>",raw),1) #grab the first place it occurs
index_end <- ifelse(length(grep("</ul>",raw))<index,length(raw),tail(grep("</ul>",raw))) #grab the last place it occurs
links <- ifelse(length(index)>0,c(raw[c(index:index_end)]),"")
index <- grep("<a href", links)  #ifelse(length(links)>0,grep("<a href", links), "")
links <- ifelse(length(index)<1, "",
read_html(unlist(links[[index]])) %>%
html_nodes("a") %>%
html_attr("href")
)
} links <- paste0(" "))
#This function is hideous but it works
extract <- function(x,y) {
#x <- positions[8] #for testing
#y <- positions[9] #for testing
#counter <- x-5
#add membership group
group <- "blank"
#Add their name
name <- str_replace_all(members[x],c("<h4>" = "", "</h4>" = ""))
#Then set the position to one back from the heading, and to 2 back from the NEXT heading
x <- x-1
y <- y-2
#And extract just the relevant chunk for this member
raw <- members[x:y]
#Grab the picture if there is one
pic <- ifelse(grepl("img",members[x]),members[x],paste0("no pic"))
#Find the p bio, and grab that
index <- grep("<p>",raw)
bio <- c(raw[c(index)])
bio <- paste(bio, collapse="\n")
bio <- html_text(read_html(bio))
short_bio <- str_extract(bio, '.*?[a-z0-9][.?!](?= )')
#Find the bit with the links in it
ifelse(grepl("<ul>",raw), {
index <- head(grep("<ul>",raw),1) #grab the first place it occurs
index_end <- ifelse(length(grep("</ul>",raw))<index,length(raw),tail(grep("</ul>",raw))) #grab the last place it occurs
links <- ifelse(length(index)>0,c(raw[c(index:index_end)]),"")
index <- grep("<a href", links)  #ifelse(length(links)>0,grep("<a href", links), "")
links <- ifelse(length(index)<1, "",
read_html(unlist(links[[index]])) %>%
html_nodes("a") %>%
html_attr("href")
)
},
links <- paste0(" "))
#Find the UTS profile, href for UTS
index <- grep("uts.edu.au",links)
profile <- ifelse(length(index)<1,paste0(""),links[c(index)])
#Find the twitter handle, href for twitter
index <- grep("twitter",links)
twitter <- ifelse(length(index)<1,paste0(""),links[c(index)])
#and then return all of that...this is VERY inelegant but it should work
person <- as.data.frame(cbind(group,name,pic,bio,short_bio,profile,twitter))
person %>%
mutate(across(everything(), as.character))
overall <<- rbind(overall,person)
}
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
View(overall)
positions <- c(grep("(<h4>)\\w+",unlist(list.members[2])),length(unlist(list.members[2]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
positions[6:7]
x <- positions[6] #for testing
y <- positions[7]
#Add their name
name <- str_replace_all(members[x],c("<h4>" = "", "</h4>" = ""))
name
members[1]
unlist(members)
raw_site <- divs %>% as.character()
list.members <- list()
for (h in heads) {
b <- strsplit(raw_site,h,fixed=T)
c <- strsplit(b[[1]][1],'\n', fixed=T)
c <- list(c[[1]][c[[1]] != ""])
# add vector of member to list
list.members <- c(list.members, c)
# update text
raw_site <- b[[1]][2]
}
# remove first element of main list
list.members <- list.members[2:length(list.members)]
# add final segment of raw.text to list
c <- strsplit(raw_site, '\n', fixed=TRUE)
c <- list(c[[1]][c[[1]] != ""])
list.members <- c(list.members, c)
#remove the 5th element
list.members <- list.members[1:4]
list.members
positions <- c(grep("(<h4>)\\w+",unlist(list.members[2])),length(unlist(list.members[2]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
members <- unlist(list.members[2])
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
#And extract just the relevant chunk for this member
raw <- members[x:y]
raw
#Grab the picture if there is one
pic <- ifelse(grepl("img",members[x]),members[x],paste0("no pic"))
#Find the p bio, and grab that
index <- grep("<p>",raw)
bio <- c(raw[c(index)])
bio <- paste(bio, collapse="\n")
bio <- html_text(read_html(bio))
short_bio <- str_extract(bio, '.*?[a-z0-9][.?!](?= )')
pic
bio
#Find the bit with the links in it
ifelse(grepl("<ul>",raw), {
index <- head(grep("<ul>",raw),1) #grab the first place it occurs
index_end <- ifelse(length(grep("</ul>",raw))<index,length(raw),tail(grep("</ul>",raw))) #grab the last place it occurs
links <- ifelse(length(index)>0,c(raw[c(index:index_end)]),"")
index <- grep("<a href", links)  #ifelse(length(links)>0,grep("<a href", links), "")
links <- ifelse(length(index)<1, "",
read_html(unlist(links[[index]])) %>%
html_nodes("a") %>%
html_attr("href")
)
},
links <- paste0(" "))
#Find the UTS profile, href for UTS
index <- grep("uts.edu.au",links)
profile <- ifelse(length(index)<1,paste0(""),links[c(index)])
#Find the twitter handle, href for twitter
index <- grep("twitter",links)
twitter <- ifelse(length(index)<1,paste0(""),links[c(index)])
#and then return all of that...this is VERY inelegant but it should work
person <- as.data.frame(cbind(group,name,pic,bio,short_bio,profile,twitter))
#############FOR ASSOCIATES ################
positions <- c(grep("(<h4>)\\w+",unlist(list.members[2])),length(unlist(list.members[2]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
positions
members <- unlist(list.members[2])
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
##############HONORARIES #################
positions <- c(grep("(<h4>)\\w+",unlist(list.members[3])),length(unlist(list.members[3]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
members <- unlist(list.members[3])
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
overall$group <- c(rep(" associate member",length(grep("(<h4>)\\w+",unlist(list.members[3]))))) #add member group for n of people in it
#Create a dataframe
overall <- data.frame(group=character(),
name=character(),
pic=character(),
bio=character(),
short_bio=character(),
profile=character(),
twitter=character(),
stringsAsFactors=FALSE)
###########FOR THE MEMBERS ###########
positions <- c(grep("(<h4>)\\w+",unlist(list.members[1])),length(unlist(list.members[1]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
members <- unlist(list.members[1])
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
overall$group <- c(rep("member",length(grep("(<h4>)\\w+",unlist(list.members[1]))))) #add member group for n of people in it
##############HONORARIES #################
positions <- c(grep("(<h4>)\\w+",unlist(list.members[3])),length(unlist(list.members[3]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
members <- unlist(list.members[3])
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
overall$group <- c(overall$group,rep("honorary associate member",length(grep("(<h4>)\\w+",unlist(list.members[3]))))) #add member group for n of people in it
overall$group
overall$group <- str_replace_all(overall$group, "blank", "honorary associate member")
overall$group
############### HDRS #######################
positions <- c(grep("(<h4>)\\w+",unlist(list.members[4])),length(unlist(list.members[4]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
members <- unlist(list.members[4])
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
overall$group <- str_replace_all(overall$group, "blank", "Higher Degree Research Student")
#############FOR ASSOCIATES ################
positions <- c(grep("(<h4>)\\w+",unlist(list.members[2])),length(unlist(list.members[2]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
#Create a dataframe
overall <- data.frame(group=character(),
name=character(),
pic=character(),
bio=character(),
short_bio=character(),
profile=character(),
twitter=character(),
stringsAsFactors=FALSE)
#This function is hideous but it works
extract <- function(x,y) {
#x <- positions[8] #for testing
#y <- positions[6] #for testing
#counter <- x-5
#add membership group
group <- "blank"
#Add their name
name <- str_replace_all(members[x],c("<h4>" = "", "</h4>" = ""))
#Then set the position to one back from the heading, and to 2 back from the NEXT heading
x <- x-1
#Grab the picture if there is one
pic <- ifelse(grepl("img",members[x]),members[x],paste0("no pic"))
#y <- ifelse(y-x < 3, y, y-2)  #This matters only if there if there is a
#And extract just the relevant chunk for this member
raw <- members[x:y]
#Find the p bio, and grab that
index <- grep("<p>",raw)
bio <- c(raw[c(index)])
bio <- paste(bio, collapse="\n")
bio <- html_text(read_html(bio))
short_bio <- str_extract(bio, '.*?[a-z0-9][.?!](?= )')
#Find the bit with the links in it
ifelse(grepl("<ul>",raw), {
index <- head(grep("<ul>",raw),1) #grab the first place it occurs
index_end <- ifelse(length(grep("</ul>",raw))<index,length(raw),tail(grep("</ul>",raw))) #grab the last place it occurs
links <- ifelse(length(index)>0,c(raw[c(index:index_end)]),"")
index <- grep("<a href", links)  #ifelse(length(links)>0,grep("<a href", links), "")
links <- ifelse(length(index)<1, "",
read_html(unlist(links[[index]])) %>%
html_nodes("a") %>%
html_attr("href")
)
},
links <- paste0(" "))
#Find the UTS profile, href for UTS
index <- grep("uts.edu.au",links)
profile <- ifelse(length(index)<1,paste0(""),links[c(index)])
#Find the twitter handle, href for twitter
index <- grep("twitter",links)
twitter <- ifelse(length(index)<1,paste0(""),links[c(index)])
#and then return all of that...this is VERY inelegant but it should work
person <- as.data.frame(cbind(group,name,pic,bio,short_bio,profile,twitter))
person %>%
mutate(across(everything(), as.character))
overall <<- rbind(overall,person)
}
###########FOR THE MEMBERS ###########
positions <- c(grep("(<h4>)\\w+",unlist(list.members[1])),length(unlist(list.members[1]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
members <- unlist(list.members[1])
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
overall$group <- c(rep("member",length(grep("(<h4>)\\w+",unlist(list.members[1]))))) #add member group for n of people in it
##############HONORARIES #################
positions <- c(grep("(<h4>)\\w+",unlist(list.members[3])),length(unlist(list.members[3]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
members <- unlist(list.members[3])
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
overall$group <- str_replace_all(overall$group, "blank", "honorary associate member")
############### HDRS #######################
positions <- c(grep("(<h4>)\\w+",unlist(list.members[4])),length(unlist(list.members[4]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
members <- unlist(list.members[4])
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
overall1 <- overall1
overall1 <- overall
#############FOR ASSOCIATES ################
positions <- c(grep("(<h4>)\\w+",unlist(list.members[2])),length(unlist(list.members[2]))) #where are the H4s for this group, and what's the final position in the document (end point for last person's anchor)
members <- unlist(list.members[2])
map2(positions[1:length(positions)-1],positions[2:length(positions)], extract) #function to extract
overall$group <- str_replace_all(overall$group, "blank", "associate member")
rm(overall1)
overall$pic
overall$pic[20]
?html_element
html_elements(read_html(overall$pic[20]),"img src")
html_elements(read_html(overall$pic[20]),"picture")
as.character(html_elements(read_html(overall$pic[20]),"picture"))
html_elements(read_html(overall$pic[20]),"img")
html_elements(read_html(overall$pic[20]),"img") %>% html_attr("data-src")
html_elements(read_html(overall$pic[20]),"img") %>% html_attr("img-src")
html_elements(read_html(overall$pic[20]),"img") %>% html_attr("a")
html_elements(read_html(overall$pic[20]),"img") %>% html_attr("href")
read_html(overall$pic[20])) %>%
html_node(xpath = '//*/img') %>%
html_attr('src')
read_html(overall$pic[20]) %>%
html_node(xpath = '//*/img') %>%
html_attr('src')
overall$pic_url <- lapply(overall$pic, function(x) {
ifelse(grep("no pic", x, "no pic", {
read_html(x) %>%
html_node(xpath = '//*/img') %>%
html_attr('src')
}))
})
overall$pic_url <- lapply(overall$pic, function(x) {
ifelse(grepl("no pic", x), "no pic", {
read_html(x) %>%
html_node(xpath = '//*/img') %>%
html_attr('src')
})
})
head(overall)
?apply
colnames(overall)
overall["name"]
overall[1]
overall[,1]
overall[1,]
x <- overall[1,] #for testing
#x <- overall[1,] #for testing
ifelse(sapply(list.files("/content/authors", full.names=TRUE), FUN=function(x){
@
#x <- overall[1,] #for testing
ifelse(sapply(list.files("/content/authors", full.names=TRUE), FUN=function(x){
grepl(x["name"], readLines(x))
}),
paste0("yes"), paste0("no"))
sapply(list.files("/content/authors", full.names=TRUE), FUN=function(x){
grepl(x["name"], readLines(x))
})
sapply(list.files("/content/authors", full.names=TRUE), FUN=function(y){
grepl(x["name"], readLines(y))
})
sapply(list.files("/content/authors", full.names=TRUE), FUN=function(y){
grepl("Simon Knight", readLines(y))
})
?list.files
#x <- overall[1,] #for testing
ifelse(sapply(list.files("/content/authors", full.names=TRUE, recursive=TRUE), FUN=function(x){
grepl(x["name"], readLines(x))
}), paste0("yes"), paste0("no"))
sapply(list.files("/content/authors", full.names=TRUE, recursive=TRUE), FUN=function(x){
grepl(x["name"], readLines(x))
})
list.files("/content/authors",full.names=T,recursive=T)
getwd
getwd()
list.files("content/authors",full.names=T,recursive=T)
sapply(list.files("content/authors", full.names=TRUE, recursive=TRUE), FUN=function(x){
grepl(x["name"], readLines(x))
})
sapply(list.files("content/authors", full.names=TRUE, recursive=TRUE), FUN=function(x){
grepl("Simon Knight", readLines(x))
})
sapply(list.files("content/authors", full.names=TRUE, recursive=TRUE), FUN=function(y){
grepl("Simon Knight", readLines(y))
})
sapply(list.files("content/authors", full.names=TRUE, recursive=TRUE), FUN=function(y){
grep("Simon Knight", readLines(y))
})
sum(sapply(list.files("content/authors", full.names=TRUE, recursive=TRUE), FUN=function(y){
grepl("Simon Knight", readLines(y))
}))
sapply(list.files("content/authors", full.names=TRUE, recursive=TRUE), FUN=function(y){
grep("Simon Knight", readLines(y))
})
?list.files
list.files("content/authors", full.names=TRUE, recursive=TRUE, pattern="index.md")
colnames(overall)
#x <- overall[1,] #for testing
ifelse(sapply(list.files("content/authors", full.names=TRUE, recursive=TRUE, pattern="index.md"),
FUN=function(y){
grepl(x[2], readLines(y))
}),
paste0("yes"), paste0("no"))
sapply(list.files("content/authors", full.names=TRUE, recursive=TRUE, pattern="index.md"),
FUN=function(y){
grepl(x[2], readLines(y))
})
x[2]
x[[2]]
sapply(list.files("content/authors", full.names=TRUE, recursive=TRUE, pattern="index.md"),
FUN=function(y){
grep(x[[2]], readLines(y))
}),
paste0("yes"), paste0("no"))
sapply(list.files("content/authors", full.names=TRUE, recursive=TRUE, pattern="index.md"),
FUN=function(y){
grep(x[[2]], readLines(y))
})
list.files("content/authors", full.names=TRUE, recursive=TRUE, pattern="index.md")
grep("Simon Knight", readlines(c( "content/authors/admin/_index.md","content/authors/Simon-Knight/_index.md")))
grep("Simon Knight", readLines(c( "content/authors/admin/_index.md","content/authors/Simon-Knight/_index.md")))
fil <- list.files("content/authors", full.names=TRUE, recursive=TRUE, pattern="index.md")
sapply(fil, function(y) { grep("Simon Knight",readLines(y))})
paste0(sapply(fil, function(y) { grep("Simon Knight",readLines(y))}))
table(sapply(fil, function(y) { grepl("Simon Knight",readLines(y))}))
sapply(fil, function(y) { grepl("Simon Knight",readLines(y))})
sapply(fil, function(y) { any(grepl("Simon Knight",readLines(y))}))
sapply(fil, function(y) { any(grepl("Simon Knight",readLines(y)))})
any(sapply(fil, function(y) { any(grepl("Simon Knight",readLines(y)))}))
grep(x[[2]], readLines("content/authors/Simon-Knight/_index.md"))
grep("Simon Knight", readLines("content/authors/Simon-Knight/_index.md"))
grep(paste0(x[[2]]), readLines("content/authors/Simon-Knight/_index.md"))
grep(paste0(x[2]), readLines("content/authors/Simon-Knight/_index.md"))
grep(paste0(unlist(x[2])), readLines("content/authors/Simon-Knight/_index.md"))
grep((unlist(x[2])), readLines("content/authors/Simon-Knight/_index.md"))
grep((unlist(x[[2]])), readLines("content/authors/Simon-Knight/_index.md"))
x[2]
paste0(x[2])
x <- overall[2,] #for testing
grep("Simon Knight", readLines("content/authors/Simon-Knight/_index.md"))
grepl("Simon Knight", readLines("content/authors/Simon-Knight/_index.md"))
any(grepl("Simon Knight", readLines("content/authors/Simon-Knight/_index.md")))
x[2]
paste(x[2],sep="-")
?gsub
gsub(" ","-",paste(x[2]))
dir.create(paste0("content/authors/",gsub(" ","-",paste(x[2]))))
dir.create(paste0("content\\authors\\",gsub(" ","-",paste(x[2]))))
?create.dir
?dir.create
dir.create(paste0(".content\\authors\\",gsub(" ","-",paste(x[2]))))
gitcreds::getcreds_set()
gitcreds::gitcreds_set()
gitcreds::gitcreds_set()
